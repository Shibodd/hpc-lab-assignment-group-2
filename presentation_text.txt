What does the program do?
- Computes three matrix multiplications,
   the last of which is the product of the other two results.

Where can the program be parallelized?
- Data parallelism: each matrix multiplicaton operation is embarassly parallel
   and can thus be parallelized with any number of threads.
  Expected Amdahl speedup: 4 (p=1, s=N_cores=4)

- Task parallelism: the first two multiplications may be executed in parallel,
   but the third has to run after the first two are complete.
  Expected Amdahl speedup: 1.5 (p=2/3, s=2)


What is our approach?
- We only exploit Data Parallelism because it exposes work for any number of cores.
  The Task Parallelism exposes work for only two threads, which doesn't scale.


Sequential program:
- [show results]


Program 1:
- Exploit the Data Parallelism in matrix multiplications
   using #pragma omp parallel and #pragma omp for.
  Each thread computes a portion of the result rows.

  We have a low IPC, which suggests we might be poorly accessing memory.
  Using perf record and report we see that most of the time is spent on loads and stores.

  [ show results ]


Program 2:
- Optimize the data accesses of the previous program.
  1) Accumulate each result element in a local before writing it to memory.
  2) Transpose the R.H.S. matrices before performing the multiplication to expose
      locality due to row-major storage.

  [ show results ]